% A LaTeX (non-official) template for ISAE projects reports
% Copyright (C) 2014 Damien Roque
% Version: 0.2
% Author: Damien Roque <damien.roque_AT_isae.fr>

\documentclass[a4paper,12pt]{book}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
%\usepackage[frenchb]{babel} % If you write in French
\usepackage[english]{babel} % If you write in English
\usepackage{a4wide}
\usepackage{graphicx}
\graphicspath{{images/}}
\usepackage{subfig}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\newlength\figureheight
\newlength\figurewidth
\pgfkeys{/pgf/number format/.cd,
set decimal separator={,\!},
1000 sep={\,},
}
\usepackage{ifthen}
\usepackage{ifpdf}
\ifpdf
\usepackage[pdftex]{hyperref}
\else
\usepackage{hyperref}
\fi
\usepackage{color}
\hypersetup{%
colorlinks=true,
linkcolor=black,
citecolor=black,
urlcolor=black}

\renewcommand{\baselinestretch}{1.05}
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyfoot{}
\fancyhead[LE,RO]{\bfseries\thepage}
\fancyhead[RE]{\bfseries\nouppercase{\leftmark}}
\fancyhead[LO]{\bfseries\nouppercase{\rightmark}}
\setlength{\headheight}{15pt}

\let\headruleORIG\headrule
\renewcommand{\headrule}{\color{black} \headruleORIG}
\renewcommand{\headrulewidth}{1.0pt}
\usepackage{colortbl}
\arrayrulecolor{black}

\fancypagestyle{plain}{
  \fancyhead{}
  \fancyfoot[C]{\thepage}
  \renewcommand{\headrulewidth}{0pt}
}

\makeatletter
\def\@textbottom{\vskip \z@ \@plus 1pt}
\let\@texttop\relax
\makeatother

\makeatletter
\def\cleardoublepage{\clearpage\if@twoside \ifodd\c@page\else%
  \hbox{}%
  \thispagestyle{empty}%
  \newpage%
  \if@twocolumn\hbox{}\newpage\fi\fi\fi}
\makeatother

\usepackage{amsthm}
\usepackage{amssymb,amsmath,bbm}
\usepackage{array}
\usepackage{bm}
\usepackage{multirow}
\usepackage[footnote]{acronym}
\usepackage{float}
\usepackage{amsmath}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}



\newcommand*{\SET}[1]  {\ensuremath{\mathbf{#1}}}
\newcommand*{\VEC}[1]  {\ensuremath{\boldsymbol{#1}}}
\newcommand*{\FAM}[1]  {\ensuremath{\boldsymbol{#1}}}
\newcommand*{\MAT}[1]  {\ensuremath{\boldsymbol{#1}}}
\newcommand*{\OP}[1]  {\ensuremath{\mathrm{#1}}}
\newcommand*{\NORM}[1]  {\ensuremath{\left\|#1\right\|}}
\newcommand*{\DPR}[2]  {\ensuremath{\left \langle #1,#2 \right \rangle}}
\newcommand*{\calbf}[1]  {\ensuremath{\boldsymbol{\mathcal{#1}}}}
\newcommand*{\shift}[1]  {\ensuremath{\boldsymbol{#1}}}

\newcommand{\eqdef}{\stackrel{\mathrm{def}}{=}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\ud}{\, \mathrm{d}}
\newcommand{\vect}{\text{Vect}}
\newcommand{\sinc}{\ensuremath{\mathrm{sinc}}}
\newcommand{\esp}{\ensuremath{\mathbb{E}}}
\newcommand{\hilbert}{\ensuremath{\mathcal{H}}}
\newcommand{\fourier}{\ensuremath{\mathcal{F}}}
\newcommand{\sgn}{\text{sgn}}
\newcommand{\intTT}{\int_{-T}^{T}}
\newcommand{\intT}{\int_{-\frac{T}{2}}^{\frac{T}{2}}}
\newcommand{\intinf}{\int_{-\infty}^{+\infty}}
\newcommand{\Sh}{\ensuremath{\boldsymbol{S}}}
\newcommand{\C}{\SET{C}}
\newcommand{\R}{\SET{R}}
\newcommand{\Z}{\SET{Z}}
\newcommand{\N}{\SET{N}}
\newcommand{\K}{\SET{K}}
\newcommand{\reel}{\mathcal{R}}
\newcommand{\imag}{\mathcal{I}}
\newcommand{\cmnr}{c_{m,n}^\reel}
\newcommand{\cmni}{c_{m,n}^\imag}
\newcommand{\cnr}{c_{n}^\reel}
\newcommand{\cni}{c_{n}^\imag}
\newcommand{\tproto}{g}
\newcommand{\rproto}{\check{g}}
\newcommand{\LR}{\mathcal{L}_2(\SET{R})}
\newcommand{\LZ}{\ell_2(\SET{Z})}
\newcommand{\LZI}[1]{\ell_2(\SET{#1})}
\newcommand{\LZZ}{\ell_2(\SET{Z}^2)}
\newcommand{\diag}{\operatorname{diag}}
\newcommand{\noise}{z}
\newcommand{\Noise}{Z}
\newcommand{\filtnoise}{\zeta}
\newcommand{\tp}{g}
\newcommand{\rp}{\check{g}}
\newcommand{\TP}{G}
\newcommand{\RP}{\check{G}}
\newcommand{\dmin}{d_{\mathrm{min}}}
\newcommand{\Dmin}{D_{\mathrm{min}}}
\newcommand{\Image}{\ensuremath{\text{Im}}}
\newcommand{\Span}{\ensuremath{\text{Span}}}

\newtheoremstyle{break}
  {11pt}{11pt}%
  {\itshape}{}%
  {\bfseries}{}%
  {\newline}{}%
\theoremstyle{break}

%\theoremstyle{definition}
\newtheorem{definition}{Définition}[chapter]

%\theoremstyle{definition}
\newtheorem{theoreme}{Théorème}[chapter]

%\theoremstyle{remark}
\newtheorem{remarque}{Remarque}[chapter]

%\theoremstyle{plain}
\newtheorem{propriete}{Propriété}[chapter]
\newtheorem{exemple}{Exemple}[chapter]

\parskip=5pt
%\sloppy

\begin{document}
\SweaveOpts{concordance=TRUE}

%%%%%%%%%%%%%%%%%%
%%% First page %%%
%%%%%%%%%%%%%%%%%%

\begin{titlepage}
\begin{center}

\includegraphics[width=0.6\textwidth]{logo}\\[1cm]

{\large The Wharton School, University of Pennsylvania}\\[0.5cm]

{\large A project report on}\\[0.5cm]

% Title
\rule{\linewidth}{0.5mm} \\[0.4cm]
{ \huge \bfseries Sentiment Analysis after US elections \\[0.4cm] }
\rule{\linewidth}{0.5mm} \\[1.5cm]

% Author and supervisor
\noindent
\begin{minipage}{0.4\textwidth}
  \begin{flushleft} \large
    \emph{Author :}\\
    Rahul Dhakecha
  \end{flushleft}
\end{minipage}%
\begin{minipage}{0.4\textwidth}
  \begin{flushright} \large
    \emph{Professor :} \\
    Dr. Linda Zhao 
  \end{flushright}
\end{minipage}

\vfill

% Bottom of the page
{\large Course: STAT 571, Modern Data Mining\\ \today}

\end{center}
\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Non-significant pages %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\frontmatter

\chapter*{Summary}
“Statistics and data science gets more credit than it deserves when it’s correct—and more blame than it deserves when it’s incorrect.” - Anthony Goldbloom, CEO of Kaggle.

This statement was more than validated on November 8th 2016, witnessing mixed emotions spread throughout the country, with few people being extremely happy while others deeply disheartened. But the most serious blow was to the statisticians and data science community. We know Data Science went wrong in the 2016 US elections. Well, it was a prediction and predictions might go wrong. Should we cease to rely on such techniques? Should we stop relying on weather forecasts and other such crucial applications of learning and data science? Ofcourse, we know the answer. Data science banks on tons of assumptions, few being extremely naive. These assumptions may sometimes come out completely wrong, which eventually leads to the failure of data science. Nevertheless, data science is competent enough to give the gist of the happenings around the world.

In this report, we analyze the sentiments of people for our president elect Donald Trump. This report is mainly divided into two parts. First part analyzes the variation in sentiments of people over a period of past one month, from November 8th to December 8th 2016. Second part focuses on more recent sentiments of people, from December 10th to December 17th 2016. Second part is further broken down to analyze common sentiments over different states of the US.

\clearpage
\tableofcontents

\clearpage
\listoffigures

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Content of the report and references %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mainmatter
\pagestyle{fancy}

\cleardoublepage

<<r1,echo=FALSE,include=FALSE>>=
library(devtools)
library(twitteR)           # library for handling twitter data
library(plyr)              # library for data manipulation
library(dplyr)             # library for data manipulation
library(tidytext)
library(stringr)
library(ggplot2)
library(reshape2)
### Text mining packages
library(tm) # major text mining package
# https://cran.r-project.org/web/packages/tm/tm.pdf
library(SnowballC) # for stemming

### Word cloud packages
library(RColorBrewer)
library(wordcloud)

### RTextTools
library(RTextTools)

# Package for sentiment analysis
install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)

install_github("mannau/tm.plugin.sentiment")
library(tm.plugin.sentiment)
@




\chapter{Part I}

As mentioned earlier, first part of analysis focuses on the variation of sentiments across one month. For the purpose of sentiment analysis, data is downloaded from Twitter, one of the largest microblogging site in terms of users. 

\section{Data Acquisition}

Twitter API provides an easy access for tweets to be downloaded on R platform. But this access is limited to the tweets from past one week only. It becomes a non trivial task to access a month old tweets. Packages like rvest in R are very handy to scrap through online content but owing to the flexibility and availability of Python repositories, Twitter data in this part of the report is accessed with the help of Python. 

Github repository[link here] was modified to access the specific contents for this report. Tweets were filtered based on the keyword and time frame. Tweeets were encoded using utf-8 encoding which is compatible with R. Further these tweets were saved as new line separated content in a single text file. Following snapshot provides the commands used for the above mentioned steps.

\begin{figure}[H]
\centering
\includegraphics[scale=1.0]{snapshot1.png}
\caption{Python code}
\label{fig:2}
\end{figure}

In the above code snippet, 1000 tweets from November 8th 2016 to December 8th 2016 which contains the keyword $"kudlow"$ is accessed and stored in a text file named $"text\_kudlow"$, with new line character separating each tweet. This text file is then directly accessed through R code.

<< r2,echo=FALSE,include=FALSE >>=
data_tweets_08_Nov <- read.table("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_08_Nov.txt",header = FALSE, sep = "\n", quote = "", colClasses = "character", fill=TRUE, strip.white = TRUE, blank.lines.skip = TRUE, comment.char = "", stringsAsFactors = F)
data_tweets_01_Dec <- read.table("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_01_Dec.txt",header = FALSE, sep = "\n", quote = "", colClasses = "character", fill=TRUE, strip.white = TRUE, blank.lines.skip = TRUE, comment.char = "", stringsAsFactors = F)
data_tweets_obama_speech <- read.table("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_obama_speech.txt",header = FALSE, sep = "\n", quote = "", colClasses = "character", fill=TRUE, strip.white = TRUE, blank.lines.skip = TRUE, comment.char = "", stringsAsFactors = F)
data_tweets_bannon <- read.table("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_bannon.txt",header = FALSE, sep = "\n", quote = "", colClasses = "character", fill=TRUE, strip.white = TRUE, blank.lines.skip = TRUE, comment.char = "", stringsAsFactors = F)
data_tweets_kudlow <- read.table("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_kudlow.txt",header = FALSE, sep = "\n", quote = "", colClasses = "character", fill=TRUE, strip.white = TRUE, blank.lines.skip = TRUE, comment.char = "", stringsAsFactors = F)
@

In the similar fashion, we develop five different files summarized in following table.

\begin{center}
\begin{tabular}{ |C{1.9cm}|C{1.9cm}|C{1.9cm}|C{1.9cm}|C{1.9cm}|C{1.9cm}| } 
 \hline
 File name & Description & Keyword & Since & Until & number \\ 
 \hline
 data\_tweets \_08\_Nov & for sentiment analysis immediately after election & trump & 11/08/2016 & 11/16/2016 & 5000 \\
 \hline
 data\_tweets \_01\_Dec & for sentiment analysis after one month of election & trump & 12/01/2016 & 12/08/2016 & 5000 \\
  \hline
 data\_tweets \_obama \_speech & for sentiment analysis after Obama's speech & obama speech & 11/08/2016 & 12/08/2016 & 1000 \\
  \hline
 data\_tweets \_bannon & after nomination of Bannon & bannon & 11/08/2016 & 12/08/2016 & 1000 \\
  \hline
 data\_tweets \_kudlow & after nomination of Kudlow & kudlow & 11/08/2016 & 12/08/2016 & 1000 \\
 \hline
\end{tabular}
\end{center}


Since this sample, in any significant way, does not represent the true population of the US, limitations of this analysis is addressed in the concluding remarks. Files obatained above are raw tweets obtained from Twitter and they need to be cleaned before using it for sentiment analysis.


\section{Data Cleaning}

Tweets obtained in raw format has variety of contents such as punctuations, url, etc which does not contribute to any significant sentiment and thus it needs to be removed. Also, we need to remove few essential elements like emojis, numbers, etc; which may contribute to the sentiment analysis but for simplicity we drop its usage.

Following steps are followed to clean the data and bring it into analyzable form.
\begin{itemize}
\item Data obtained from the text file is accessed in the form of table content and is needed to be converted into character strings. 
\item This list is then converted into corpus, which contains two parts- character string and metadata.
\item Majority of tweets have URL attached to them, which are removed as they do not contribute any significant information in sentiment analysis.
\item Many tweets are addressed to someone and this information is redundant in analysis; therefore all target elements are removed.
\item For simplicity, effect of emoticons(emojis) in this analysis is not considered. All emojis are converted into hex codes, which are removed. This assumption is one of the naive assumption as emojis convey a lot of information about the sentiment of a tweet. 
\item Entire text is converted into lower case, eliminated english stopwords, punctuations and number. Removal of english stop words like "not","nor" and "no" is again a naive step as these words convey critical information about sentiment of a particular treat. For example, a sentence like "People are not happy" is a negative statement but it will be classified as a positive sentiment after removal of english stopwords.
\item When a tweet text file is created using a particular keyword, the appearance of that keyword is gauranteed in all the tweets. Such words are removed which inspite of occuring frequently does not carry any significant amount of information.
\item Finally all the words are stemmed to retain their significance, and at the same time dropped of the extra content of the word.
\end{itemize}


<< r3,echo=FALSE,include=FALSE >>=
## DATA PREPROCESSING

##converting table form data into character strings
data_tweets_08_Nov_mod <- as.list(sapply(data_tweets_08_Nov, as.character))

## converting character text data into list
mycorpus1 <- VCorpus( VectorSource(data_tweets_08_Nov_mod))

## removing URL from text
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
mycorpus2  <- tm_map(mycorpus1, content_transformer(removeURL))

## removing target elements
removeTarget <- function(x) gsub("@\\w+", "", x)
mycorpus3  <- tm_map(mycorpus2, content_transformer(removeTarget))

# this step is used for encoding our characters to the desired character type. If this is not used, then modifications of few elements of corpus creates unusual problems. Here 'UTF-8-MAC' is a type of encoding used and 'byte' indicates one character string. This part is indirectly handling emoticons.
mycorpus4 <- tm_map(mycorpus3,
                     content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
                     mc.cores=1)

## converting entire text into lower case
mycorpus5 <- tm_map(mycorpus4, content_transformer(tolower),lazy = TRUE)

## remove stopwords
mycorpus6<- tm_map(mycorpus5, removeWords, stopwords("english"),lazy = TRUE)

## remove puntuations
mycorpus7 <- tm_map(mycorpus6, removePunctuation,lazy = TRUE)

## remove numbers
mycorpus8 <- tm_map(mycorpus7, removeNumbers,lazy = TRUE)

mycorpus9 <- tm_map(mycorpus8, removeWords, c("US","election","trump","httpswww","http","httpwww","www","elect","https","uselections","elections","donald","link","bannon","steve","via","larry","kudlow","will","the"))

## stem words
mycorpus10_one <- tm_map(mycorpus9, stemDocument, lazy = TRUE)    # steming converts words like "elections"" to "elect" and therefore needs to be done prior to removal of common words

# convert corpus to a Plain Text Document
corpus_elect_result=tm_map(mycorpus10_one,PlainTextDocument,lazy = TRUE)
@

After following the above mentioned steps we store the cleaned version of tweets as a plain text document in $corpus\_elect\_result$. This variable temporarily holds data for all the different text files mentioned in the section above.

\section{Analysis}
\subsection{Wordcloud}

Data rendered after data cleaning contains all significant words. To get the gist of frequently occuirng words in the tweets, a word cloud is plotted. This word cloud is obtained from $data\_tweets\_08\_Nov$.txt file.

\begin{figure}[H]
\centering

<< r4,echo=FALSE, include=TRUE, fig=TRUE >>=
## word frequency matrix
dtm1 <- DocumentTermMatrix( mycorpus10_one )

set.seed(1)
col=brewer.pal(6,"Dark2")
wc <- wordcloud(mycorpus10_one, min.freq = 25, max.words =100, scale=c(4,.1), random.order = FALSE,rot.per=0.5,colors=col,random.color = T)
@
\caption{Wordcloud for }
\label{fig:1}
\end{figure}

This word cloud pretty much reflects the sentiments of people during the week following the results of election. While few states were fully supporting the Republican elect Donald Trump, states like California and New York had protests for number of days. Presence of words like "support","like", "for win", etc clearly displays strong favor for the president elect while words like "protest", "hate", "racist", etc clearly shows strong disagreement of people for the results of US elections.

\subsection{Sentiments immediately after US elections}

To further dive into the text corpus, we try to segregate our analysis based on various sentiments like trust, anger, joy, etc. This analysis is carried out with the help of "syuzhet" package developed by Stanford NLP team. It implements Saif Mohammad’s NRC Emotion lexicon. According to Mohammad, “the NRC emotion lexicon is a list of words and their associations with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). 



For the sake of visual clearity, we represent negative sentiment by red color and positive sentiment by green color. We divide all the words in the corpus into different sentiment categories and the resultant proportion is plotted on bar graph. The plot reveals that immediately after the results, majority of population was gripped in the wave of fear. Owing to the president elect's bold statements against minorities, Muslims and women, this fear was more than obvious. Large chunks of population came out on streets to express their griefs and anger. But at the same time, there was major proportion of population which expressed their trust on the president elect. This trust owed to the promises made by Republican candidate on creating new jobs and his ability to drive profitable businesses. Overall it can be seen that negative sentiments surpassed positive sentiments during the period following immediately after election results.


\begin{figure}[!]
\centering
<< r5,echo=FALSE,include=TRUE, fig=TRUE >>=
library(syuzhet)
syuzhet_vector <- get_sentiment(as.character(corpus_elect_result), method="syuzhet")

nrc_data <- get_nrc_sentiment(as.character(corpus_elect_result))    #returns different types of emotions for each of our text

barplot(
  sort(colSums(prop.table(nrc_data[, 1:8]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  col=c("green","green","red","green","red","red","green","red"),
  main = "Emotions immediately after the US elections", xlab="Percentage"
  )
@
\caption{Loss functions}
\label{fig:1}
\end{figure}


\subsection{Sentiments one month past US elections}
It can be seen that over the period of one month, "trust" has replaced "fear" as the maximum proportionate sentiment among people. Nevertheless, this change is not significant by a huge margin. But it definitely shows that people are coming in support for new government irrespective of the differences in their thoughts and philosophy. 



<< r6,echo=FALSE,include=TRUE, fig=TRUE >>=
## DATA PREPROCESSING

##converting table form data into character strings
data_tweets_01_Dec_mod <- as.list(sapply(data_tweets_01_Dec, as.character))

## converting character text data into list
mycorpus1 <- VCorpus( VectorSource(data_tweets_01_Dec_mod))

## removing URL from text
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
mycorpus2  <- tm_map(mycorpus1, content_transformer(removeURL))

## removing target elements
removeTarget <- function(x) gsub("@\\w+", "", x)
mycorpus3  <- tm_map(mycorpus2, content_transformer(removeTarget))

# this step is used for encoding our characters to the desired character type. If this is not used, then modifications of few elements of corpus creates unusual problems. Here 'UTF-8-MAC' is a type of encoding used and 'byte' indicates one character string. This part is indirectly handling emoticons.
mycorpus4 <- tm_map(mycorpus3,
                     content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
                     mc.cores=1)

## converting entire text into lower case
mycorpus5 <- tm_map(mycorpus4, content_transformer(tolower),lazy = TRUE)

## remove stopwords
mycorpus6<- tm_map(mycorpus5, removeWords, stopwords("english"),lazy = TRUE)

## remove puntuations
mycorpus7 <- tm_map(mycorpus6, removePunctuation,lazy = TRUE)

## remove numbers
mycorpus8 <- tm_map(mycorpus7, removeNumbers,lazy = TRUE)

mycorpus9 <- tm_map(mycorpus8, removeWords, c("US","election","trump","httpswww","http","httpwww","www","elect","https","uselections","elections","donald","link","bannon","steve","via","larry","kudlow","will","the"))

## stem words
mycorpus10 <- tm_map(mycorpus9, stemDocument, lazy = TRUE)    # steming converts words like "elections"" to "elect" and therefore needs to be done prior to removal of common words

# convert corpus to a Plain Text Document
corpus_elect_result=tm_map(mycorpus10,PlainTextDocument,lazy = TRUE)


syuzhet_vector <- get_sentiment(as.character(corpus_elect_result), method="syuzhet")

nrc_data <- get_nrc_sentiment(as.character(corpus_elect_result))    #returns different types of emotions for each of our text

barplot(
  sort(colSums(prop.table(nrc_data[, 1:8]))),
  horiz = TRUE,
  cex.names = 0.7,
  las = 1,
  col=c("green", "green", "red","green", "red", "red","red","green"),
  main = "Emotions one month after the US elections", xlab="Percentage"
  )
@


Also, it can be seen that there is still around 30 percent of people expressing sadness and anger in some or the other way. One month after the results has witnessed several major and minor events which has swayed people's emotions. We take a look at few of the prominent ones and try to reflect the sentiments based on the word cloud.

\subsection{Sentiments after Obama's speech over President elect}

<< r7,echo=FALSE,include=TRUE,warning=FALSE, fig=TRUE >>=
##converting table form data into character strings
data_tweets_obama_speech_mod <- as.list(sapply(data_tweets_obama_speech, as.character))

## converting character text data into list
mycorpus1 <- VCorpus( VectorSource(data_tweets_obama_speech_mod))

## removing URL from text
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
mycorpus2  <- tm_map(mycorpus1, content_transformer(removeURL))

## removing target elements
removeTarget <- function(x) gsub("@\\w+", "", x)
mycorpus3  <- tm_map(mycorpus2, content_transformer(removeTarget))

# this step is used for encoding our characters to the desired character type. If this is not used, then modifications of few elements of corpus creates unusual problems. Here 'UTF-8-MAC' is a type of encoding used and 'byte' indicates one character string. This part is indirectly handling emoticons.
mycorpus4 <- tm_map(mycorpus3,
                     content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
                     mc.cores=1)

## converting entire text into lower case
mycorpus5 <- tm_map(mycorpus4, content_transformer(tolower),lazy = TRUE)

## remove stopwords
mycorpus6<- tm_map(mycorpus5, removeWords, stopwords("english"),lazy = TRUE)

## remove puntuations
mycorpus7 <- tm_map(mycorpus6, removePunctuation,lazy = TRUE)

## remove numbers
mycorpus8 <- tm_map(mycorpus7, removeNumbers,lazy = TRUE)

mycorpus9 <- tm_map(mycorpus8, removeWords, c("US","election","trump","httpswww","http","httpwww","www","elect","https","uselections","elections","donald","link","bannon","steve","via","larry","kudlow","will","the","obama","speech","presid","obamaspeech","president"))

## stem words
mycorpus10 <- tm_map(mycorpus9, stemDocument, lazy = TRUE)    # steming converts words like "elections"" to "elect" and therefore needs to be done prior to removal of common words

# convert corpus to a Plain Text Document
corpus_elect_result=tm_map(mycorpus10,PlainTextDocument,lazy = TRUE)

set.seed(1)
col=brewer.pal(6,"Dark2")
wordcloud(mycorpus10, min.freq = 25, max.words =100, scale=c(4,.1), random.order = FALSE,rot.per=0.5,colors=col,random.color = T)
@

Despite strong differences in opinions on matter of sheer importance, current president Barack Obama gave congratulatory speech on Novemeber 9th at White House. This speech addressed the positivies of the new government and was in the direction of soothing the angry mob. He assured smooth transition of government and assured befitting atmosphere for the coming generations. Enhancing the spirit of patriotism, this speech definitely wooed the mass.

\subsection{Sentiments after appointment of Steve Bannon as chief White House strategist}

<< r8,echo=FALSE,include=TRUE,warning=FALSE, fig=TRUE >>=
##converting table form data into character strings
data_tweets_bannon_mod <- as.list(sapply(data_tweets_bannon, as.character))

## converting character text data into list
mycorpus1 <- VCorpus( VectorSource(data_tweets_bannon_mod))

## removing URL from text
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
mycorpus2  <- tm_map(mycorpus1, content_transformer(removeURL))

## removing target elements
removeTarget <- function(x) gsub("@\\w+", "", x)
mycorpus3  <- tm_map(mycorpus2, content_transformer(removeTarget))

# this step is used for encoding our characters to the desired character type. If this is not used, then modifications of few elements of corpus creates unusual problems. Here 'UTF-8-MAC' is a type of encoding used and 'byte' indicates one character string. This part is indirectly handling emoticons.
mycorpus4 <- tm_map(mycorpus3,
                     content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
                     mc.cores=1)

## converting entire text into lower case
mycorpus5 <- tm_map(mycorpus4, content_transformer(tolower),lazy = TRUE)

## remove stopwords
mycorpus6<- tm_map(mycorpus5, removeWords, stopwords("english"),lazy = TRUE)

## remove puntuations
mycorpus7 <- tm_map(mycorpus6, removePunctuation,lazy = TRUE)

## remove numbers
mycorpus8 <- tm_map(mycorpus7, removeNumbers,lazy = TRUE)

mycorpus9 <- tm_map(mycorpus8, removeWords, c("US","election","trump","httpswww","http","httpwww","www","elect","https","uselections","elections","donald","link","bannon","steve","via","larry","kudlow","will","the"))

## stem words
mycorpus10 <- tm_map(mycorpus9, stemDocument, lazy = TRUE)    # steming converts words like "elections"" to "elect" and therefore needs to be done prior to removal of common words

# convert corpus to a Plain Text Document
corpus_elect_result=tm_map(mycorpus10,PlainTextDocument,lazy = TRUE)

set.seed(1)
col=brewer.pal(6,"Dark2")
wordcloud(mycorpus10, min.freq = 25, max.words =100, scale=c(4,.1), random.order = FALSE,rot.per=0.5,colors=col,random.color = T)
@

Steve Bannon was CEO of Donald Trump's presidential campaign and was later appointed as Chief Strategist at White House on November 13th 2016. This appointment saw severe opposition from many groups especially from Anti Defamation League and Council on American Islamic relations. Bannon is often considered racist, white nationalist and anti Semitic. Words like "white", "antisemit", "racist", etc portray this polarity in Bannon's view. Words like "stop","don't", etc displays negative public opinion towards this appointment.


\subsection{Sentiments after appointment of Larry Kudlow as Chairman of Council of Economic Advisor}

<< r9,echo=FALSE,include=TRUE,warning=FALSE, fig=TRUE >>=
##converting table form data into character strings
data_tweets_kudlow_mod <- as.list(sapply(data_tweets_kudlow, as.character))

## converting character text data into list
mycorpus1 <- VCorpus( VectorSource(data_tweets_kudlow_mod))

## removing URL from text
removeURL <- function(x) gsub("http[[:alnum:][:punct:]]*", "", x)
mycorpus2  <- tm_map(mycorpus1, content_transformer(removeURL))

## removing target elements
removeTarget <- function(x) gsub("@\\w+", "", x)
mycorpus3  <- tm_map(mycorpus2, content_transformer(removeTarget))

# this step is used for encoding our characters to the desired character type. If this is not used, then modifications of few elements of corpus creates unusual problems. Here 'UTF-8-MAC' is a type of encoding used and 'byte' indicates one character string. This part is indirectly handling emoticons.
mycorpus4 <- tm_map(mycorpus3,
                     content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub='byte')),
                     mc.cores=1)

## converting entire text into lower case
mycorpus5 <- tm_map(mycorpus4, content_transformer(tolower),lazy = TRUE)

## remove stopwords
mycorpus6<- tm_map(mycorpus5, removeWords, stopwords("english"),lazy = TRUE)

## remove puntuations
mycorpus7 <- tm_map(mycorpus6, removePunctuation,lazy = TRUE)

## remove numbers
mycorpus8 <- tm_map(mycorpus7, removeNumbers,lazy = TRUE)

mycorpus9 <- tm_map(mycorpus8, removeWords, c("US","election","trump","httpswww","http","httpwww","www","elect","https","uselections","elections","donald","link","bannon","steve","via","larry","kudlow","will","the"))

## stem words
mycorpus10 <- tm_map(mycorpus9, stemDocument, lazy = TRUE)    # steming converts words like "elections"" to "elect" and therefore needs to be done prior to removal of common words

# convert corpus to a Plain Text Document
corpus_elect_result=tm_map(mycorpus10,PlainTextDocument,lazy = TRUE)

set.seed(1)
col=brewer.pal(6,"Dark2")
wordcloud(mycorpus10, min.freq = 25, max.words =100, scale=c(4,.1), random.order = FALSE,rot.per=0.5,colors=col,random.color = T)
@

Another major decision which raised eyes was president elect's nomination of Larry Kudlow as Chairman of Council of Economic Advisor. While Kudlow does not have controversies following him as Steve Bannon, but his academic inexperience leaves many loop holes to be filled. Position which was once chaired by great economists such as Janet Yellen, Ben Bernanke and Alan Greenspan demands a lot of intellect and decisive power. Words like "idiot", "danger", etc reflects a sense of doubt in public.

\chapter{Part II}
In this part we directly access the data from Twitter API without involving Python. We set authorization with twitter and save the accessed data.

\section{Data Acquisition}

We download the twitter data for 10 prominent states of the US in terms of population and economy. We also pay special attention to the swing states Michigan, Pennsylvania, Florida, Ohio, Iowa, North Carolina and Wisconsin which were the deciding states in 2016 US elections. For each state, we take its capital, feed its geographical information in twitter and get 1000 tweets within a radius of 100 miles. 

<< r10,include=TRUE,echo=FALSE >>=
## Twitter authorization
api_key <- "5f78aJsDfMxciR3zfcS14m8AX"
api_secret <- "7KlkUP1pkYZj9jGoy3odsjNTd324OStKHEBiPjWEKzrIzSufnY"
access_token <- "806704739509501952-dHvqMMJomZaYc7kZFsB72M5jA9T7rQe"
access_token_secret <- "ZKAC6qJXoVMi1vxo5no2zGVXuHF2YHDRuDAROGwpuJpnF"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)
@

\section{Analysis}

\subsection{New York sentiment}

In this part, we list all positive words along with their corresponding counts. For this analysis we use "tidytext" package which gives us further flexibility to handle text data. Inherent structure of this package allows us to create different groups of sentiments. This package handles sentiments with the help of three lexicons:
\begin{itemize}
\item AFFIN - This lexicon rates different words on the scale of -5 to +5, where more negative score representing negative sentiments and vice versa.
\item Bing - This lexicon divides all the words into either positive or negative.
\item nrc - This package divides all the words into 8 different sentiments, similar to that of "syuzhet" package.
\end{itemize}
Using bing lexicon, we create bar graph representing different positive and negative sentiments.

<< r11,echo=FALSE,include=TRUE,warning=FALSE >>=
## downloading tweets
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_NY.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

nrc_senti <- tidy_tweets %>%
  semi_join(nrcjoy) %>%
  count(word, sort = TRUE)
@

\subsection{Positive Negative Words}

This section segregates positive and negative words from the tweet data set.

<< r12,echo=FALSE,include=TRUE >>=
tidy_tweets %>%
  count(word, sort = TRUE)

bing <- get_sentiments("bing")
bing_word_counts <- tidy_tweets %>%         ##this gives list of positive and negative words
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# graph showing the words which contribute most to the recent happenings related to US election results
bing_word_counts %>%
  filter(n > 10) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")
@

\subsection{Wordcloud positive negative words}

<< r13,echo=FALSE,include=TRUE, fig=TRUE >>=

# word cloud
tidy_tweets %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
@

\subsection{Positive Negative Words}

Finally we count total number of negative and positive words in New York state. We see that bing lexicon has total of 4782 negative words and 2006 positive words. Therefore, number of negative count will always be greater than positive count. To mitigate this effect, we will multiply the negative count by a factor of 2006/4782. This multiplication will scale down negative counts to the level of positive counts.

<< r14,echo=FALSE,include=TRUE >>=
#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_NY <- sum(bing_pos_count$n)
count_pos_NY

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_NY <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_NY
@


In similar fashion, we analyse positive and negative word counts for different states and we find overall shift towards positive sentiments, which validates our conclusion of part I.

<< r15,echo=FALSE,include=FALSE >>=

#---------------------------------CALIFORNIA---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_cal.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_cal <- sum(bing_pos_count$n)
count_pos_cal

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_cal <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_cal

#---------------------------------MICHIGAN---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_mich.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_mich <- sum(bing_pos_count$n)
count_pos_mich

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_mich <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_mich

#---------------------------------FLORIDA---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_florida.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_florida <- sum(bing_pos_count$n)
count_pos_florida

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_florida <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_florida

#---------------------------------PENNSYLVANIA---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_penn.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_penn <- sum(bing_pos_count$n)
count_pos_penn

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_penn <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_penn

#-------------------------------------ILLINOIS-----------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_illinois.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_illinois <- sum(bing_pos_count$n)
count_pos_illinois

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_illinois <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_illinois

#-------------------------------------WISCONSIN---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_WI.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_WI <- sum(bing_pos_count$n)
count_pos_WI

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_WI <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_WI

#---------------------------------NORTH CAROLINA---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_NC.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_NC <- sum(bing_pos_count$n)
count_pos_NC

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_NC <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_NC

#--------------------------------------OHIO---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_ohio.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_ohio <- sum(bing_pos_count$n)
count_pos_ohio

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_ohio <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_ohio

#---------------------------------IOWA---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_iowa.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_iowa <- sum(bing_pos_count$n)
count_pos_iowa

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_iowa <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_iowa

#--------------------------------------VIRGINIA---------------------------------------------#
tweets <- read.csv("/Users/rahuldhakecha/coursesfall2016/moderndatamining/finalproject/tweets_VA.csv")

reg <- "([^A-Za-z_\\d#@']|'(?![A-Za-z_\\d#@]))"
tidy_tweets <- tweets %>% 
  mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|http://[A-Za-z\\d]+|&amp;|&lt;|&gt;|RT", "")) %>%
  unnest_tokens(word, text, token = "regex", pattern = reg) %>%
  filter(!word %in% stop_words$word,
         str_detect(word, "[a-z]"))

#count total number of positives and negatives in a given state
bing_pos <- get_sentiments("bing") %>% 
  filter(sentiment == "positive")
bing_pos_count <- tidy_tweets %>%
  inner_join(bing_pos) %>%
  count(word, sentiment, sort = TRUE)
count_pos_VA <- sum(bing_pos_count$n)
count_pos_VA

bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
bing_neg_count <- tidy_tweets %>%
  inner_join(bing_neg) %>%
  count(word, sentiment, sort = TRUE)
count_neg_VA <- as.integer(sum(bing_neg_count$n)*(2006/4782))
count_neg_VA
@

\section*{References}
\begin{enumerate}
\item https://github.com/mayank93/Twitter-Sentiment-Analysis/tree/master/docs
\item https://cran.r-project.org/web/packages/syuzhet/syuzhet.pdf
\item https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html
\end{enumerate}




\end{document}